import pyaudio
import webrtcvad
import wave
import threading
import time
import collections
import requests
import json
import os
from queue import Queue
import sounddevice as sd
import soundfile as sf
from funasr import AutoModel

# === å‚æ•°é…ç½®éƒ¨åˆ† ===
RATE = 16000
CHANNELS = 1
FORMAT = pyaudio.paInt16
CHUNK_MS = 30
CHUNK_SIZE = int(RATE * CHUNK_MS / 1000)
VAD_MODE = 2
SILENCE_TIMEOUT = 3.0
OUTPUT_FILE = "recorded.wav"

# é˜Ÿåˆ—
tts_q = Queue()
player_q = Queue()

# å¤–éƒ¨ä¿¡å·
recording_signal = False
flag_path = "/tmp/flag.txt"

# ğŸ”´ å…¨å±€æ‰“æ–­äº‹ä»¶
interrupt_event = threading.Event()
first_wakeup_done = False  # åˆ¤æ–­ç¬¬ä¸€æ¬¡å”¤é†’

# ============ å½•éŸ³ç±» ============
class MicRecorder:
    def __init__(self):
        self.vad = webrtcvad.Vad(VAD_MODE)
        self.pa = pyaudio.PyAudio()
        self.stream = self.pa.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            input=True,
            frames_per_buffer=CHUNK_SIZE
        )

    def is_speech(self, frame_bytes):
        return self.vad.is_speech(frame_bytes, RATE)

    def record_once(self):
        print("[*] ç­‰å¾…å½•éŸ³ä¿¡å·...")
        while not recording_signal:
            if interrupt_event.is_set():
                return None
            time.sleep(0.1)

        print("[+] å½•éŸ³å¼€å§‹")
        frames = []
        silent_chunks = collections.deque(maxlen=int(SILENCE_TIMEOUT * 1000 / CHUNK_MS))

        while True:
            if interrupt_event.is_set():
                print("[-] å½•éŸ³è¢«æ‰“æ–­")
                return None
            frame = self.stream.read(CHUNK_SIZE, exception_on_overflow=False)
            if self.is_speech(frame):
                frames.append(frame)
                silent_chunks.clear()
            else:
                if frames:
                    silent_chunks.append(frame)
                    if len(silent_chunks) == silent_chunks.maxlen:
                        frames.extend(silent_chunks)
                        print("[-] é™éŸ³è¶…æ—¶ï¼Œå½•éŸ³ç»“æŸ")
                        break

        self._save_wav(frames)
        return OUTPUT_FILE

    def _save_wav(self, frames):
        with wave.open(OUTPUT_FILE, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(self.pa.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))

    def close(self):
        self.stream.stop_stream()
        self.stream.close()
        self.pa.terminate()

# ============ è¯­éŸ³è½¬æ–‡æœ¬ ============
def transcribe_with_funasr(filepath, stt_model):
    print("[*] æ­£åœ¨è¯†åˆ«éŸ³é¢‘å†…å®¹...")
    res = stt_model.generate(input=filepath, data_type="sound", inference_clip_length=250, disable_update=True)
    word_text = res[0]['text']
    print("[è¯†åˆ«ç»“æœ]:", word_text)
    return word_text

# ============ AI å“åº”ï¼ˆRAG APIï¼‰ ============
def AI_response(user_text, conversation_id):
    API_URL = "http://192.168.166.144/v1/chat-messages"
    API_KEY = "app-MAzgaqOFk2jgM7wspctReZeG"
    user = "ShowRoom"
    headers = {
        "Content-Type": "application/json",
        "Accept": "tex
